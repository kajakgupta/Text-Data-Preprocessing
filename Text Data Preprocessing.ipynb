{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text didn't aren't</i>\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "[Some text we don't want to keep is in here]\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "[This is some other unwanted text]\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "&nbsp;&nbsp;\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "22    45   1067   445\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "{Here is more stuff in single curly braces.}\n",
    "[DELETE]\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text didn't aren't\n",
      "\n",
      "But this will still be here!\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "  \n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "22    45   1067   445\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "sample = denoise_text(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text did not are not\n",
      "\n",
      "But this will still be here!\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "  \n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "22    45   1067   445\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\" replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "sample = replace_contractions(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'did', 'not', 'are', 'not', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', 'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kajal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is done to execute the above tokeniserTN\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'did', 'not', 'are', 'not', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', 'Sebastian', ',', 'Nicolas', ',', 'Alejandro', 'and', 'Jeronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', 'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non- Ascii character from list of tokenized word\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "w = remove_non_ascii(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'goes', 'here', 'bolded', 'text', 'italicized', 'text', 'did', 'not', 'are', 'not', 'but', 'this', 'will', 'still', 'be', 'here', '!', 'i', 'run', '.', 'he', 'ran', '.', 'she', 'is', 'running', '.', 'will', 'they', 'stop', 'running', '?', 'i', 'talked', '.', 'she', 'was', 'talking', '.', 'they', 'talked', 'to', 'them', 'about', 'running', '.', 'who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡sebastián', ',', 'nicolás', ',', 'alejandro', 'and', 'jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'i', 'can', 'not', 'do', 'this', 'anymore', '.', 'i', 'did', 'not', 'know', 'them', '.', 'why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'my', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'indiana', 'jones', ';', 'marvel', 'cinematic', 'universe', ';', 'star', 'wars', ';', 'back', 'to', 'the', 'future', ';', 'harry', 'potter', '.', 'do', 'not', 'do', 'it', '...', '.', 'just', 'do', 'not', '.', 'billy', '!', 'i', 'know', 'what', 'you', 'are', 'doing', '.', 'this', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'john', ':', '``', 'well', ',', 'well', ',', 'well', '.', \"''\", 'james', ':', '``', 'there', ',', 'there', '.', 'there', ',', 'there', '.', \"''\", 'there', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'there', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'i', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"concert all the characters to lowercase from list of tokenizer words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "w = to_lowercase(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'did', 'not', 'are', 'not', 'But', 'this', 'will', 'still', 'be', 'here', 'I', 'run', 'He', 'ran', 'She', 'is', 'running', 'Will', 'they', 'stop', 'running', 'I', 'talked', 'She', 'was', 'talking', 'They', 'talked', 'to', 'them', 'about', 'running', 'Who', 'ran', 'to', 'the', 'talking', 'runner', 'Sebastián', 'Nicolás', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', 'something', 'is', 'wrong', 'with', 'this', 'sentence', 'I', 'can', 'not', 'do', 'this', 'anymore', 'I', 'did', 'not', 'know', 'them', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', 'My', 'favorite', 'movie', 'franchises', 'in', 'order', 'Indiana', 'Jones', 'Marvel', 'Cinematic', 'Universe', 'Star', 'Wars', 'Back', 'to', 'the', 'Future', 'Harry', 'Potter', 'do', 'not', 'do', 'it', 'Just', 'do', 'not', 'Billy', 'I', 'know', 'what', 'you', 'are', 'doing', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', 'John', 'Well', 'well', 'well', 'James', 'There', 'there', 'There', 'there', 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '1000000', 'reasons', 'actually', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', 'too', '22', '45', '1067', '445', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces']\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\" remove punctuation from list of tokenizer words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "w = remove_punctuation(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    \"\"\"replace all intergers occurance in list of tokenizer word with textual reperesentation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in w:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "            #print(new_words)\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    #print(new_words)\n",
    "    return new_words\n",
    "\n",
    "#aw = replace_numbers(words)\n",
    "#print(aw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'five'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check if inflect is working.\n",
    "a=5\n",
    "\n",
    "p= inflect.engine()\n",
    "b = p.number_to_words(a)\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hundred and one\n",
      "one million\n",
      "two\n",
      "two\n",
      "twenty-two\n",
      "forty-five\n",
      "one thousand and sixty-seven\n",
      "four hundred and forty-five\n"
     ]
    }
   ],
   "source": [
    "# to check all integer and print their word form\n",
    "p = inflect.engine()\n",
    "for w in words:\n",
    "    n = w.isdigit()\n",
    "    if n!= False:\n",
    "        a = p.number_to_words(w)\n",
    "        print(a)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not is stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'still', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'running', '.', 'Will', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'talking', '.', 'They', 'talked', 'running', '.', 'Who', 'ran', 'talking', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'Jéronimo', 'going', 'store', 'tomorrow', 'morning', '!', 'something', '...', '!', 'wrong', '(', ')', 'with.', ',', ';', ':', ':', 'sentence', '.', 'I', 'anymore', '.', 'I', 'know', '.', 'Why', 'could', 'dinner', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'Future', ';', 'Harry', 'Potter', '.', '...', '.', 'Just', '.', 'Billy', '!', 'I', 'know', '.', 'This', 'great', 'little', 'house', 'got', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', '.', 'There', ',', '.', \"''\", 'There', 'lot', 'reasons', '.', 'There', '101', 'reasons', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'go', 'get', '2', 'tutus', '2', 'different', 'stores', ',', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'stuff', 'inside', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'stuff', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "w = remove_stopwords(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kajal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is done to execute the above stopwords\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['titl', 'goe', 'her', 'bold', 'text', 'it', 'text', 'did', 'not', 'ar', 'not', 'but', 'thi', 'wil', 'stil', 'be', 'her', '!', 'i', 'run', '.', 'he', 'ran', '.', 'she', 'is', 'run', '.', 'wil', 'they', 'stop', 'run', '?', 'i', 'talk', '.', 'she', 'was', 'talk', '.', 'they', 'talk', 'to', 'them', 'about', 'run', '.', 'who', 'ran', 'to', 'the', 'talk', 'run', '?', '¡sebastián', ',', 'nicolá', ',', 'alejandro', 'and', 'jéronimo', 'ar', 'going', 'to', 'the', 'stor', 'tomorrow', 'morn', '!', 'someth', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'thi', ':', ':', 'sent', '.', 'i', 'can', 'not', 'do', 'thi', 'anym', '.', 'i', 'did', 'not', 'know', 'them', '.', 'why', 'could', 'not', 'you', 'hav', 'din', 'at', 'the', 'resta', '?', 'my', 'favorit', 'movy', 'franch', ',', 'in', 'ord', ':', 'indian', 'jon', ';', 'marvel', 'cinem', 'univers', ';', 'star', 'war', ';', 'back', 'to', 'the', 'fut', ';', 'harry', 'pot', '.', 'do', 'not', 'do', 'it', '...', '.', 'just', 'do', 'not', '.', 'bil', '!', 'i', 'know', 'what', 'you', 'ar', 'doing', '.', 'thi', 'is', 'a', 'gre', 'littl', 'hous', 'you', 'hav', 'got', 'her', '.', 'john', ':', '``', 'wel', ',', 'wel', ',', 'wel', '.', \"''\", 'jam', ':', '``', 'ther', ',', 'ther', '.', 'ther', ',', 'ther', '.', \"''\", 'ther', 'ar', 'a', 'lot', 'of', 'reason', 'not', 'to', 'do', 'thi', '.', 'ther', 'ar', '101', 'reason', 'not', 'to', 'do', 'it', '.', '1000000', 'reason', ',', 'act', '.', 'i', 'hav', 'to', 'go', 'get', '2', 'tut', 'from', '2', 'diff', 'stor', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'her', 'is', 'som', 'stuff', 'insid', 'of', 'doubl', 'cur', 'brac', '.', '}', '}', '{', 'her', 'is', 'mor', 'stuff', 'in', 'singl', 'cur', 'brac', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "w = stem_words(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'do', 'not', 'be', 'not', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'run', '.', 'She', 'be', 'run', '.', 'Will', 'they', 'stop', 'run', '?', 'I', 'talk', '.', 'She', 'be', 'talk', '.', 'They', 'talk', 'to', 'them', 'about', 'run', '.', 'Who', 'run', 'to', 'the', 'talk', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'be', 'go', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'be', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'do', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchise', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', 'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'be', 'do', '.', 'This', 'be', 'a', 'great', 'little', 'house', 'you', 'have', 'get', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'be', 'a', 'lot', 'of', 'reason', 'not', 'to', 'do', 'this', '.', 'There', 'be', '101', 'reason', 'not', 'to', 'do', 'it', '.', '1000000', 'reason', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'store', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'be', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'brace', '.', '}', '}', '{', 'Here', 'be', 'more', 'stuff', 'in', 'single', 'curly', 'brace', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize verbs in list of tokenized words\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "w = lemmatize_verbs(words)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kajal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = normalize(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-92c1432f17f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mstems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_and_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stemmed:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nLemmatized:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-92c1432f17f5>\u001b[0m in \u001b[0;36mstem_and_lemmatize\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstem_and_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize_verbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-b8c5af005aac>\u001b[0m in \u001b[0;36mstem_words\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mstems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\lancaster.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \"\"\"\n\u001b[0;32m    210\u001b[0m         \u001b[1;31m# Lower-case the word, since all the rules are lower-cased\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__stripPrefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strip_prefix\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Calling the stemming and lemming functions are done as below:\\\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "stems, lemmas = stem_and_lemmatize(words)\n",
    "print('Stemmed:\\n', stems)\n",
    "print('\\nLemmatized:\\n', lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
